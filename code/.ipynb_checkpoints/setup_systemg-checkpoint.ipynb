{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Setup System G\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "def deg2rad(deg):\n",
    "    return deg/360*(2*np.pi)\n",
    "\n",
    "def append_m_prefix(row):\n",
    "    prefix = 'm'\n",
    "    new_row={}\n",
    "    new_row['mid'] = prefix+str(row['m_id'])\n",
    "    new_row['m_id'] = row['m_id']\n",
    "    return new_row\n",
    "\n",
    "def append_z_prefix(row):\n",
    "    prefix = 'z'\n",
    "    new_row={}\n",
    "    new_row['zid'] = prefix+str(row['z_id'])\n",
    "    new_row['z_id'] = row['z_id']\n",
    "    return new_row\n",
    "\n",
    "PATH_SYSTEMG = '../systemg/'\n",
    "PATH_RAWDATA = '../rawdata/'\n",
    "PATH_PROCESSEDDATA = '../processeddata/'\n",
    "METRO_FN = 'Gaz_ua_national.txt'\n",
    "ZCTA_FN = 'Gaz_zcta_national.txt'\n",
    "METRO_CSV_FN = 'metroareas.csv'\n",
    "ZCARBON_FN = 'zcarbon.csv'\n",
    "NODES_FN = 'nodes.csv'\n",
    "DRIV_DIST_FN='driv_dist.parquet'\n",
    "DRIV_DIST_CSV_FN='driv_dist.parquet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfMetroAreasRaw = spark.read.load(PATH_RAWDATA+METRO_FN, format=\"csv\", delimiter=\"\\t\", header=True, inferSchema=True)\n",
    "dfMetroAreasRaw = dfMetroAreasRaw.withColumnRenamed(dfMetroAreasRaw.columns[-1],dfMetroAreasRaw.columns[-1].strip(string.whitespace))\n",
    "dfMetroAreas = dfMetroAreasRaw.select('GEOID','NAME','UATYPE','POP10','HU10','ALAND_SQMI',\\\n",
    "                                      'AWATER_SQMI','INTPTLAT','INTPTLONG')\\\n",
    "                            .withColumnRenamed('GEOID','m_id')\\\n",
    "                            .withColumnRenamed('NAME', 'name')\\\n",
    "                            .withColumnRenamed('POP10','m_pop')\\\n",
    "                            .withColumnRenamed('HU10','m_house_unit')\\\n",
    "                            .withColumnRenamed('ALAND_SQMI','m_land')\\\n",
    "                            .withColumnRenamed('AWATER_SQMI','m_water')\\\n",
    "                            .withColumnRenamed('INTPTLAT', 'm_lat_d')\\\n",
    "                            .withColumnRenamed('INTPTLONG', 'm_long_d')\n",
    "\n",
    "\n",
    "dfMetroAreas = dfMetroAreas.withColumn('m_lat_r', deg2rad(dfMetroAreas.m_lat_d)).withColumn('m_long_r', deg2rad(dfMetroAreas.m_long_d))\n",
    "\n",
    "temp = dfMetroAreas.rdd.map(append_m_prefix)\n",
    "dfR=spark.createDataFrame(temp)\n",
    "    \n",
    "dfMetroAreas = dfR.join(dfMetroAreas,dfMetroAreas.m_id==dfR.m_id).drop(dfR.m_id)\n",
    "\n",
    "dfMetroAreas.coalesce(1).write.csv(PATH_SYSTEMG+METRO_CSV_FN,header=True,mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfZData = spark.read.csv(PATH_PROCESSEDDATA + ZCARBON_FN,header=True)\n",
    "temp = dfZData.rdd.map(append_z_prefix)\n",
    "dfR=spark.createDataFrame(temp)\n",
    "    \n",
    "dfZData = dfR.join(dfZData,dfZData.z_id==dfR.z_id).drop(dfR.z_id)\n",
    "dfZData.count()\n",
    "dfZData.printSchema()\n",
    "dfZData.coalesce(1).write.csv(PATH_SYSTEMG + NODES_FN,header=True,mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfDDist = spark.read.parquet(PATH_PROCESSEDDATA+DRIV_DIST_FN)\n",
    "\n",
    "temp = dfDDist.select('z_id').distinct().rdd.map(append_z_prefix)\n",
    "dfZ=spark.createDataFrame(temp)\n",
    "\n",
    "temp = dfDDist.select('m_id').distinct().rdd.map(append_m_prefix)\n",
    "dfM=spark.createDataFrame(temp)\n",
    "\n",
    "dfMD = dfM.join(dfDDist,dfDDist.m_id==dfM.m_id)\n",
    "dfDDist = dfZ.join(dfMD,dfMD.z_id==dfZ.z_id)\n",
    "\n",
    "\n",
    "dfDDist = dfDDist.select('zid','mid','ddist')\n",
    "dfDDist.coalesce(1).write.csv(PATH_SYSTEMG+DRIV_DIST_CSV_FN,header=True,mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
