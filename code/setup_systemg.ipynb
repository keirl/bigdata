{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Calculate Distances\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/session.py:336: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "def deg2rad(deg):\n",
    "    return deg/360*(2*np.pi)\n",
    "\n",
    "def append_m_prefix(row):\n",
    "    prefix = 'm'\n",
    "    new_row={}\n",
    "    new_row['mid'] = prefix+str(row['m_id'])\n",
    "    new_row['m_id'] = row['m_id']\n",
    "    return new_row\n",
    "\n",
    "dfMetroAreasRaw = spark.read.load(\"../rawdata/Gaz_ua_national.txt\", format=\"csv\", delimiter=\"\\t\", header=True, inferSchema=True)\n",
    "dfMetroAreasRaw = dfMetroAreasRaw.withColumnRenamed(dfMetroAreasRaw.columns[-1],dfMetroAreasRaw.columns[-1].strip(string.whitespace))\n",
    "dfMetroAreas = dfMetroAreasRaw.select('GEOID','NAME','UATYPE','POP10','HU10','ALAND_SQMI',\\\n",
    "                                      'AWATER_SQMI','INTPTLAT','INTPTLONG')\\\n",
    "                            .withColumnRenamed('GEOID','m_id')\\\n",
    "                            .withColumnRenamed('NAME', 'name')\\\n",
    "                            .withColumnRenamed('POP10','m_pop')\\\n",
    "                            .withColumnRenamed('HU10','m_house_unit')\\\n",
    "                            .withColumnRenamed('ALAND_SQMI','m_land')\\\n",
    "                            .withColumnRenamed('AWATER_SQMI','m_water')\\\n",
    "                            .withColumnRenamed('INTPTLAT', 'm_lat_d')\\\n",
    "                            .withColumnRenamed('INTPTLONG', 'm_long_d')\n",
    "\n",
    "\n",
    "dfMetroAreas = dfMetroAreas.withColumn('m_lat_r', deg2rad(dfMetroAreas.m_lat_d)).withColumn('m_long_r', deg2rad(dfMetroAreas.m_long_d))\n",
    "\n",
    "temp = dfMetroAreas.rdd.map(append_m_prefix)\n",
    "dfR=spark.createDataFrame(temp)\n",
    "    \n",
    "dfMetroAreas = dfR.join(dfMetroAreas,dfMetroAreas.m_id==dfR.m_id).drop(dfR.m_id)\n",
    "\n",
    "dfMetroAreas.coalesce(1).write.csv(\"/media/keir/Data/workspace/project/metroareas.csv\",header=True,mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/session.py:336: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------------+------+-------+---------+----------+-------------------+-------------------+\n",
      "| zid|z_id|z_pop|z_house_unit|z_land|z_water|  z_lat_d|  z_long_d|            z_lat_r|           z_long_r|\n",
      "+----+----+-----+------------+------+-------+---------+----------+-------------------+-------------------+\n",
      "|z601| 601|18570|        7744|64.348|  0.309|18.180555|-66.749961|0.31731054458991764|-1.1650065950278068|\n",
      "|z602| 602|41520|       18073|30.613|  1.717|18.362268| -67.17613| 0.3204820347335941|-1.1724446472477383|\n",
      "|z603| 603|54689|       25653|31.614|  0.071|18.455183|-67.119887| 0.3221037074080847|-1.1714630217165392|\n",
      "|z606| 606| 6615|        2877|42.309|  0.005|18.158345|-66.932911|0.31692290696304976|-1.1681996748943304|\n",
      "|z610| 610|29016|       12618|35.916|  1.611|18.290955|-67.125868| 0.3192373880841194| -1.171567409859101|\n",
      "+----+----+-----+------------+------+-------+---------+----------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def append_z_prefix(row):\n",
    "    prefix = 'z'\n",
    "    new_row={}\n",
    "    new_row['zid'] = prefix+str(row['z_id'])\n",
    "    new_row['z_id'] = row['z_id']\n",
    "    return new_row\n",
    "\n",
    "dfZCTAsRaw = spark.read.load(\"../rawdata/Gaz_zcta_national.txt\", format=\"csv\", delimiter=\"\\t\", header=True, inferSchema=True)\n",
    "dfZCTAsRaw = dfZCTAsRaw.withColumnRenamed(dfZCTAsRaw.columns[-1],dfZCTAsRaw.columns[-1].strip(string.whitespace))\n",
    "dfZCTAs = dfZCTAsRaw.select('GEOID','POP10','HU10','ALAND_SQMI','AWATER_SQMI','INTPTLAT','INTPTLONG')\\\n",
    "                .withColumnRenamed('GEOID','z_id')\\\n",
    "                .withColumnRenamed('POP10','z_pop')\\\n",
    "                .withColumnRenamed('HU10','z_house_unit')\\\n",
    "                .withColumnRenamed('ALAND_SQMI','z_land')\\\n",
    "                .withColumnRenamed('AWATER_SQMI','z_water')\\\n",
    "                .withColumnRenamed('INTPTLAT', 'z_lat_d')\\\n",
    "                .withColumnRenamed('INTPTLONG', 'z_long_d')\n",
    "                \n",
    "dfZCTAs = dfZCTAs.withColumn('z_lat_r', deg2rad(dfZCTAs.z_lat_d)).withColumn('z_long_r', deg2rad(dfZCTAs.z_long_d))\n",
    "\n",
    "temp = dfZCTAs.rdd.map(append_z_prefix)\n",
    "dfR=spark.createDataFrame(temp)\n",
    "\n",
    "dfZCTAs = dfR.join(dfZCTAs,dfZCTAs.z_id==dfR.z_id).drop(dfR.z_id)\n",
    "\n",
    "dfZCTAs.show(5)\n",
    "\n",
    "dfZCTAs.coalesce(1).write.csv(\"/media/keir/Data/workspace/project/zctas.csv\",header=True,mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118967040\n"
     ]
    }
   ],
   "source": [
    "# Wikipedia, \"Great-circle distance,\" https://en.wikipedia.org/wiki/Great-circle_distance retrieved 12/9/2016\n",
    "from pyspark.sql.functions import acos, cos, sin, abs\n",
    "\n",
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", True)\n",
    "\n",
    "#.where(dfMetroAreas.m_pop>20000)\n",
    "\n",
    "dfDist = dfZCTAs.select('zid','z_lat_r','z_long_r').join(dfMetroAreas.select('mid','m_lat_r','m_long_r'))\n",
    "\n",
    "RMETERS = 6371000 #meters\n",
    "RMILES = RMETERS*0.000621371\n",
    "\n",
    "dfDist = dfDist.withColumn('dist',acos(\n",
    "                            sin(dfDist.z_lat_r)*sin(dfDist.m_lat_r)+\n",
    "                            cos(dfDist.z_lat_r)*cos(dfDist.m_lat_r)*cos(abs(dfDist.z_long_r-dfDist.m_long_r))\n",
    "                            )*RMILES)\n",
    "dfDist = dfDist.select('zid','mid','dist')\n",
    "print(dfDist.count())\n",
    "dfDist.coalesce(1).write.csv(\"/media/keir/Data/workspace/project/gc_distance.csv\",header=True,mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/session.py:336: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- zid: string (nullable = true)\n",
      " |-- z_id: string (nullable = true)\n",
      " |-- z_lat_d: string (nullable = true)\n",
      " |-- z_long_d: string (nullable = true)\n",
      " |-- z_land: string (nullable = true)\n",
      " |-- z_water: string (nullable = true)\n",
      " |-- z_pop: string (nullable = true)\n",
      " |-- z_households: string (nullable = true)\n",
      " |-- z_comm: string (nullable = true)\n",
      " |-- z_med_inc: string (nullable = true)\n",
      " |-- z_house_unit: string (nullable = true)\n",
      " |-- z_comm_miles: string (nullable = true)\n",
      " |-- z_comm_miles_ph: string (nullable = true)\n",
      " |-- z_carb_ton_ph: string (nullable = true)\n",
      " |-- z_pov: string (nullable = true)\n",
      " |-- z_per_comm: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfZData = spark.read.csv(\"../systemg/nodes.csv\",header=True)\n",
    "temp = dfZData.rdd.map(append_z_prefix)\n",
    "dfR=spark.createDataFrame(temp)\n",
    "    \n",
    "dfZData = dfR.join(dfZData,dfZData.z_id==dfR.z_id).drop(dfR.z_id)\n",
    "dfZData.printSchema()\n",
    "dfZData.coalesce(1).write.csv(\"/media/keir/Data/workspace/project/nodes_2.csv\",header=True,mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26170"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfZData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/session.py:336: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "def append_m_prefix(row):\n",
    "    prefix = 'm'\n",
    "    new_row={}\n",
    "    new_row['mid'] = prefix+str(row['m_id'])\n",
    "    new_row['m_id'] = row['m_id']\n",
    "    return new_row\n",
    "\n",
    "def append_z_prefix(row):\n",
    "    prefix = 'z'\n",
    "    new_row={}\n",
    "    new_row['zid'] = prefix+str(row['z_id'])\n",
    "    new_row['z_id'] = row['z_id']\n",
    "    return new_row\n",
    "\n",
    "dfDDist = spark.read.parquet(\"../processeddata/driv_dist.parquet\")\n",
    "\n",
    "temp = dfDDist.select('z_id').distinct().rdd.map(append_z_prefix)\n",
    "dfZ=spark.createDataFrame(temp)\n",
    "\n",
    "temp = dfDDist.select('m_id').distinct().rdd.map(append_m_prefix)\n",
    "dfM=spark.createDataFrame(temp)\n",
    "\n",
    "dfMD = dfM.join(dfDDist,dfDDist.m_id==dfM.m_id)\n",
    "dfDDist = dfZ.join(dfMD,dfMD.z_id==dfZ.z_id)\n",
    "\n",
    "\n",
    "\n",
    "dfDDist = dfDDist.select('zid','mid','ddist')\n",
    "dfDDist.coalesce(1).write.csv(\"../systemg/driv_dist.csv\",header=True,mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
